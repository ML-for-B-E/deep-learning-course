{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f0675d3",
   "metadata": {},
   "source": [
    "# Classification de données de cancer du sein avec un perceptron multi-couches\n",
    "Dans ce notebook nous effectuerons l'apprentissage de données de cancer du sein pour une classification en deux classes. Le package utilisé dans ce TP pour implémenter le perceptron multi-couche est ***scikit-learn***.\n",
    "\n",
    "Pour le bon fonctionnement de ce notebook, les packages suivants sont nécessaires:\n",
    "1. *numpy*\n",
    "2. *sckit-learn*\n",
    "\n",
    "## Chargement des données et exploration\n",
    "### Chargement des données\n",
    "Les données que nous utiliserons existent déjà dans le package ***scikit-learn***. Pour les charger, nous utiliserons la fonction `load_breast_cancer`. On aurait très bien pu utiliser un fichier csv et les importer avec le package ***pandas***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be696365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5262480d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données pour la classification du cancer du sein\n",
    "cancer_data = load_breast_cancer()\n",
    "\n",
    "print(\"La base de données chargée est de type : \", type(cancer_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85fb13c",
   "metadata": {},
   "source": [
    "### Exploration des données\n",
    "Cette étape permet de pouvoir analyser les données (répartition des classes, taille, ...) afin d'avoir une idée un peu plus précise de qu'on manipule.\n",
    "Dans cet exemple, nous allons:\n",
    "- Afficher la taille de la base de données\n",
    "- Le nombre de classes \n",
    "- Le nombre d'échantillons dans chaque classe\n",
    "\n",
    "Pour faire un apprentissage, on a besoin d'avoir les données d'entrée mais également les données de sortie qui sont dans cet exemple les différentes classes. Ces deux données sont accessibles grâce aux clés `data` et `target` dans la base de données chargée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccbe1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taille des données de cancer\n",
    "print(\"Taille des données chargées: \", cancer_data['data'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c38981b",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_cancer_data_shape = (569, 30)\n",
    "np.testing.assert_allclose(cancer_data['data'].shape, expected_cancer_data_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8731ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger séparément les entrées et les sorties des données pour préparer l'apprentissage\n",
    "features = cancer_data['data']\n",
    "classes = cancer_data['target']\n",
    "\n",
    "# Afficher les différentes classes\n",
    "v_classes_uniques = np.unique(classes)\n",
    "print(len(v_classes_uniques), \"Différentes classes : \", v_classes_uniques)\n",
    "\n",
    "# Afficher la taille des données de chacune des classes\n",
    "for classe in v_classes_uniques:\n",
    "    print(\"La classe \", classe, \"contient \", np.sum(classes==classe), \"échantillons.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7933e559",
   "metadata": {},
   "source": [
    "## Préparation des données pour l'apprentissage\n",
    "### Création des ensembles d'apprentissage et de test\n",
    "Avant d'effectuer l'apprentissage, il faut diviser notre base de données en 2 :\n",
    "1. un ensemble d'apprentissage qui servira à entraîner le réseau de neurones\n",
    "C'est sur cet ensemble qu'on optimise la fonction de perte afin de minimier l'erreur de prédiction.\n",
    "2. un ensemble de test qui servira à évaluer la qualité de notre apprentissage\n",
    "Pour séparer notre ensemble de données dans ce sens, la fonction `train_test_split` est bien adaptée. Il suffit de lui indiquer la répartition qu'on souhaite en terme de ratio entre l'ensemble d'apprentissage et celui de test grâce au paramètre `train_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b7cebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, classes_train, classes_test = train_test_split(\n",
    "    features, classes, train_size=0.75, random_state=42,\n",
    ")\n",
    "\n",
    "# Afficher les tailles des deux ensembles\n",
    "print(\"Taille de l'ensemble d'apprentissage : \", features_train.shape)\n",
    "print(\"Taille de l'ensemble de test : \", features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101c961c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_allclose(features_train.shape, (426, 30))\n",
    "np.testing.assert_allclose(features_test.shape, (143, 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dc7edc",
   "metadata": {},
   "source": [
    "### Normalisation des données \n",
    "La normalisation des données est bien souvent une étape indispensable pour faciliter l'apprentissage. Cette opération est faite sur l'ensemble d'apprentisage et ensuite appliquer à l'ensemble de test et à chaque échantillon pour lequel on aimerait prédire la classe.\n",
    "\n",
    "Plusieurs normalisations sont possibles:\n",
    "- la normalisation centrée réduite\n",
    "- l'utilisation de la norme L1\n",
    "- l'utilisation de la norme L2\n",
    "- ...\n",
    "\n",
    "Nous utilisons ici la fonction `StandardScaler` dans `sklearn.prepocesing` pour faire une normalisation centrée réduite. On peut également utiliser la fonction `normalize` en passant en paramètre le type de normalisation qu'on souhaite effectuer.\n",
    "\n",
    "La normalisation ne change absolument pas la taille des échantillons mais leurs valeurs. Pour le vérifier, nous avons afficher les valeurs minimales et maximales des variances et des moyennes avant et après la normalisation centrée réduite. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6867b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation des données avant l'apprentissage\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# La normalisation se fait uniquement sur les données d'apprentissage\n",
    "scaler.fit(features_train)\n",
    "StandardScaler(copy=True , with_mean=True , with_std=True)\n",
    "\n",
    "# La normalisation ne modifie pas la taille des données mais leurs valeurs\n",
    "# Afficher les valeurs minimales et maximales de la variance et de la moyenne des données après la normalisations\n",
    "variances_train = np.std(features_train, 0)\n",
    "moyennes_train = np.mean(features_train, 0)\n",
    "\n",
    "print(\"Avant l'opération de normalisation\")\n",
    "print(\"Taille de l'ensemble d'apprentissage : \", features_train.shape)\n",
    "\n",
    "print(\"Variances des données d'apprentissage : \", [np.round(np.min(variances_train), 3), \n",
    "                                                   np.round(np.max(variances_train), 3)] )\n",
    "print(\"Moyennes des données d'apprentissage : \", [np.round(np.min(moyennes_train)), \n",
    "                                                  np.round(np.max(moyennes_train), 3)])\n",
    "\n",
    "# Application du modèle de normalisation aux données de test et d'apprentissage\n",
    "features_train = scaler.transform (features_train)\n",
    "features_test = scaler.transform (features_test)\n",
    "\n",
    "# La normalisation ne modifie pas la taille des données mais leurs valeurs\n",
    "variances_train = np.std(features_train, 0)\n",
    "moyennes_train = np.mean(features_train, 0)\n",
    "\n",
    "print(\"\\nAprès l'opération de normalisation\")\n",
    "print(\"Taille de l'ensemble d'apprentissage : \", features_train.shape)\n",
    "\n",
    "# Afficher les valeurs minimales et maximales de la variance et de la moyenne des données après la normalisations\n",
    "print(\"Variances des données d'apprentissage : \", [np.round(np.min(variances_train), 3), \n",
    "                                                   np.round(np.max(variances_train), 3)] )\n",
    "print(\"Moyennes des données d'apprentissage : \", [np.round(np.min(moyennes_train)), \n",
    "                                                  np.round(np.max(moyennes_train), 3)])\n",
    "print(variances_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78b3757",
   "metadata": {},
   "source": [
    "## Entraînement et évaluation du réseau de neurones multi-perceptron\n",
    "Pour implémenter notre réseau de neurones, nous utilisons la fonction `MLPClassifier` du module `neural_network` du package ***scikit-learn***. Cette fonction permet de paramétrer les couches cachées (nombre et taille), la fonction d'activation des couches cachées, la fonction d'optimisation, le taux d'apprentissage, le nombre maximum d'itérations, ...\n",
    "\n",
    "### Entraînement du réseau de neurones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e53272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisation d'un réseau de neurones multi-perceptron pour la classification\n",
    "\n",
    "# Taille des différentes couches cachés\n",
    "taille_couches_cachees = (30, 10)\n",
    "print(\"Nombre de couches cachées : \", len(taille_couches_cachees))\n",
    "\n",
    "# Définition du réseau de neurones\n",
    "mlp = MLPClassifier(hidden_layer_sizes=taille_couches_cachees, max_iter=2000, random_state=42)\n",
    "\n",
    "# Entrainement du réseau défini sur les données d'apprentissage\n",
    "mlp.fit(features_train, classes_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3216b42",
   "metadata": {},
   "source": [
    "### Evaluation de l'apprentissage sur les données de test\n",
    "Pour évaluer la qualité de l'apprentissage, on peut utiliser plusieurs métriques. Dans cet exemple, nous utiliserons :\n",
    "- la matrice de confusion\n",
    "- la précision\n",
    "- le recall\n",
    "- le score F1\n",
    "\n",
    "Ces différentes métriques sont déjà implémentées dans le module `metrics` du package ***scikit-learn***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1c38dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test = mlp.predict(features_test)\n",
    "\n",
    "print(\"Matrice de confusion\")\n",
    "mlp_confusion_matrix = confusion_matrix(classes_test, predictions_test)\n",
    "\n",
    "expected_mlp_confusion_matrix = np.array([[53, 1], [2, 87]])\n",
    "np.testing.assert_allclose(mlp_confusion_matrix, expected_mlp_confusion_matrix)\n",
    "\n",
    "print(confusion_matrix(classes_test, predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90391594",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(classes_test, predictions_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bf4be7",
   "metadata": {},
   "source": [
    "### Validation croisée \n",
    "La validation croissée est une approche permettant d'entraîner le modèle plusieurs fois sur des sous-échantillons de la base d'apprentissage. Elle permet de s'assurer que le modèle ne fonctionne pas uniquement dans une seule configuration de l'ensemble d'apprentissage. On peut par la suite sélectionner le modèle qui offre la meilleure précision, le meilleur recall, ...\n",
    "\n",
    "Nous avons effecutée une validation croissée avec la fonction `cross_val_score` du module `model_selction` du package ***scikit-learn***. Il faut indiquer à cette fonction :\n",
    "- le nombre de validations croisées qu'on souhaite effectuer avec le paramètre `cv`\n",
    "- la métrique d'évaluation de la validation croisée avec le paramètre `scoring`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf25dbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_cross_val_score = cross_val_score(mlp , features_train, classes_train, cv=5, scoring=\"accuracy\")\n",
    "\n",
    "expected_mlp_cross_val_score = np.array([0.96511628, 0.95294118, 0.97647059, 0.96470588, 0.97647059])\n",
    "np.testing.assert_allclose(mlp_cross_val_score, expected_mlp_cross_val_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21914593",
   "metadata": {},
   "source": [
    "---\n",
    "# Fin\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4445933b46ffea109112bd65b29d80bfcc8cc2064087c198a6c13763b109acf2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
